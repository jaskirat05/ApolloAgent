"""
Temporal Workflow Definitions for ComfyUI Execution

A Workflow orchestrates the execution flow and maintains durable state.
"""

from datetime import timedelta
from typing import Dict, Any, Optional
from dataclasses import dataclass

from temporalio import workflow
from temporalio.common import RetryPolicy

# Import activities (will be defined in activities.py)
with workflow.unsafe.imports_passed_through():
    from ..activities import (
        select_best_server,
        download_and_store_images,
        download_and_store_artifacts,
        get_server_output_files
    )
    from ..workflow_registry import get_registry


@dataclass
class WorkflowExecutionRequest:
    """Input for workflow execution"""
    workflow_definition: Dict[str, Any]
    strategy: str = "least_loaded"
    workflow_name: Optional[str] = None  # For chain execution
    server_address: Optional[str] = None  # Pre-selected server (for chain steps)
    workflow_db_id: Optional[str] = None  # Database workflow ID (for artifact tracking)


@dataclass
class WorkflowExecutionResult:
    """Result of workflow execution"""
    status: str
    prompt_id: str
    server_address: str
    output: Optional[Dict[str, Any]] = None  # Standardized output: {"video": "/path/to/file.mp4", "type": "video", ...}
    local_preview: Optional[list[Dict[str, Any]]] = None  # Local downloaded files for preview/viewing
    parameters: Optional[Dict[str, Any]] = None  # Parameters used for execution
    log_file_path: Optional[str] = None
    error: Optional[str] = None


@workflow.defn
class ComfyUIWorkflow:
    """
    Durable workflow for executing ComfyUI workflows on GPU farm

    This workflow:
    1. Selects best available GPU server
    2. Queues workflow on ComfyUI
    3. Tracks execution via WebSocket
    4. Downloads generated images
    5. Creates execution log

    All state is persisted - survives crashes and restarts.
    """

    def __init__(self):
        # Workflow state - all persisted automatically by Temporal
        self._status = "initializing"
        self._server_address: Optional[str] = None
        self._prompt_id: Optional[str] = None
        self._current_node: Optional[str] = None
        self._progress = 0.0
        self._events: list[Dict] = []
        self._error: Optional[Dict] = None
        self._cancelled = False
        self._client_id: Optional[str] = None  # ComfyUI client_id for WebSocket tracking

    @workflow.run
    async def run(self, request: WorkflowExecutionRequest) -> WorkflowExecutionResult:
        """
        Main workflow execution logic

        This entire function is durable - if the worker crashes,
        Temporal will resume from the last completed step.
        """
        workflow.logger.info(f"Starting ComfyUI workflow execution")

        try:
            # Generate unique client_id for this workflow execution
            # Use Temporal's deterministic UUID generator
            self._client_id = str(workflow.uuid4())
            workflow.logger.info(f"Generated client_id: {self._client_id}")

            # Step 1: Select best GPU server (or use pre-selected)
            self._status = "selecting_server"
            if request.server_address:
                # Use pre-selected server (from chain orchestration)
                self._server_address = request.server_address
                workflow.logger.info(f"Using pre-selected server: {self._server_address}")
            else:
                # Dynamically select server
                self._server_address = await workflow.execute_activity(
                    select_best_server,
                    request.strategy,
                    start_to_close_timeout=timedelta(seconds=30),
                    retry_policy=RetryPolicy(
                        maximum_attempts=3,
                        initial_interval=timedelta(seconds=1),
                        maximum_interval=timedelta(seconds=10),
                        backoff_coefficient=2.0
                    )
                )
                workflow.logger.info(f"Selected server: {self._server_address}")

            # Step 2: Execute workflow with new V3 client (handles queue + tracking)
            self._status = "executing"
            from ..activities import execute_and_track_workflow

            execution_result = await workflow.execute_activity(
                execute_and_track_workflow,
                args=[
                    self._server_address,
                    request.workflow_definition,
                    request.workflow_name
                ],
                start_to_close_timeout=timedelta(minutes=30),
                heartbeat_timeout=timedelta(minutes=10),
                retry_policy=RetryPolicy(
                    maximum_attempts=2,
                    initial_interval=timedelta(seconds=5),
                    maximum_interval=timedelta(seconds=30),
                    backoff_coefficient=2.0
                )
            )

            self._prompt_id = execution_result.get("prompt_id")

            # Check if execution failed
            if execution_result.get("status") == "failed":
                self._status = "failed"
                self._error = execution_result.get("error")
                workflow.logger.error(f"Execution failed: {self._error}")

                return WorkflowExecutionResult(
                    status="failed",
                    prompt_id=self._prompt_id,
                    server_address=self._server_address,
                    local_preview=[],
                    error=str(self._error)
                )

            workflow.logger.info(f"Execution completed successfully")

            # Step 3: Extract server output files from outputs
            self._status = "processing_outputs"
            from ..activities import get_server_output_files

            server_files = await workflow.execute_activity(
                get_server_output_files,
                args=[self._server_address, {"outputs": execution_result["outputs"]}],
                start_to_close_timeout=timedelta(seconds=30),
                retry_policy=RetryPolicy(
                    maximum_attempts=2,
                    initial_interval=timedelta(seconds=1),
                    maximum_interval=timedelta(seconds=5),
                    backoff_coefficient=2.0
                )
            )
            workflow.logger.info(f"Found {len(server_files)} server output file(s)")

            # Step 4: Download files locally (and persist to DB if workflow_db_id provided)
            self._status = "downloading_files"

            # Use download_and_store_artifacts if we have a workflow_db_id (chain execution)
            # Otherwise use download_and_store_images (standalone workflow)
            if request.workflow_db_id:
                workflow.logger.info(f"Using database artifact storage (workflow_id: {request.workflow_db_id})")
                downloaded_files = await workflow.execute_activity(
                    download_and_store_artifacts,
                    args=[request.workflow_db_id, self._server_address, server_files],
                    start_to_close_timeout=timedelta(minutes=5),
                    retry_policy=RetryPolicy(
                        maximum_attempts=3,
                        initial_interval=timedelta(seconds=2),
                        maximum_interval=timedelta(seconds=10),
                        backoff_coefficient=2.0
                    )
                )
            else:
                workflow.logger.info(f"Using ephemeral file storage (no workflow_db_id)")
                downloaded_files = await workflow.execute_activity(
                    download_and_store_images,
                    args=[self._server_address, server_files],
                    start_to_close_timeout=timedelta(minutes=5),
                    retry_policy=RetryPolicy(
                        maximum_attempts=3,
                        initial_interval=timedelta(seconds=2),
                        maximum_interval=timedelta(seconds=10),
                        backoff_coefficient=2.0
                    )
                )

            workflow.logger.info(f"Downloaded {len(downloaded_files)} file(s) locally")

            # Step 5: Build standardized output for chains
            output_data = None
            if request.workflow_name and server_files:
                try:
                    from ..workflow_registry import get_registry
                    registry = get_registry()
                    workflow_info = registry.get_workflow_info(request.workflow_name)

                    if workflow_info and workflow_info.get("output"):
                        output_info = workflow_info["output"]
                        output_type = output_info["output_type"]  # "video" or "image"
                        primary_file = server_files[0]["filename"] if server_files else None

                        output_data = {
                            output_type: primary_file,  # "video": "output_00001.mp4"
                            "type": output_type,
                            "format": output_info.get("format", "auto"),
                            "server_files": server_files,  # Full server file info
                            "count": len(server_files)
                        }
                except Exception as e:
                    workflow.logger.warning(f"Could not build output data: {e}")

            # Step 6: Complete
            self._status = "completed"

            return WorkflowExecutionResult(
                status="completed",
                prompt_id=self._prompt_id,
                server_address=self._server_address,
                output=output_data,
                local_preview=downloaded_files
            )

        except Exception as e:
            self._status = "failed"
            self._error = {"message": str(e), "type": type(e).__name__}
            workflow.logger.error(f"Workflow failed with error: {e}")

            return WorkflowExecutionResult(
                status="failed",
                prompt_id=self._prompt_id or "",
                server_address=self._server_address or "",
                local_preview=[],
                error=str(e)
            )

    @workflow.query
    def get_status(self) -> Dict[str, Any]:
        """
        Query to get current workflow status

        AI agents or SDK can call this anytime to get real-time state
        """
        return {
            "status": self._status,
            "server_address": self._server_address,
            "prompt_id": self._prompt_id,
            "current_node": self._current_node,
            "progress": self._progress,
            "error": self._error
        }

    @workflow.query
    def get_events(self) -> list[Dict]:
        """Get all ComfyUI WebSocket events collected so far"""
        return self._events

    @workflow.signal
    async def cancel(self):
        """
        Signal to cancel the workflow

        User or AI agent can send this to cancel execution
        """
        workflow.logger.info("Cancel signal received")
        self._cancelled = True
        self._status = "cancelled"
